#%%

from transformers import pipeline, AutoTokenizer
from typing import List, Tuple, Optional, Dict
import numpy as np
import re
import json
import csv
from pathlib import Path
import gc
import torch
import click
from _02_opensource_entity_tracking import _parse_int
from accelerate import Accelerator

# Adapted from _02_opensource_entity_tracking.py
PROMPT_TEMPLATE = (
    "You are given a short Python program involving boolean variables 'x' and 'y'. "
    "Your task is to compute the final value of the variable x. "
    "The value of x will be True or False. "
    "Return 0 if the final value of x is False, or 1 if the final value of x is True. "
    "Return only the integer (0 or 1), without commas, an equal sign, or any additional text. The integer should appear immediately after the word 'is: '.\n"
    "For example, we might have the following program: \n"
    "```python\n"
    "x = True\n"
    "y = False\n"
    "x = x and y\n"
    "x = not x\n"
    "x = x and y\n"
    "```\n"
    "The final value of x is: 0\n\n"

    "Here is the program: \n"
    "```python\n{code}\n```\n"
    "The final value of x is: "
)

DEFAULT_MODELS: List[str] = [
    # "Qwen/Qwen3-1.7B", # Smaller model for quicker testing by default
    "Qwen/Qwen3-14B",
    # Add other models as needed
]

DEFAULT_MODELS_ALL: List[str] = [
    "Qwen/Qwen3-32B",
    "Qwen/Qwen3-14B",
    "Qwen/Qwen3-8B",
    "Qwen/Qwen3-4B",
    "Qwen/Qwen3-1.7B",
    "Qwen/Qwen3-0.6B",
]

GROUPS = {
    "and": (lambda a, b: a and b, 2),
    "or": (lambda a, b: a or b, 2),
    "not": (lambda a: not a, 1),
}


def _extract_generated_part(full_text: str, prompt_text: str) -> str:
    """Extracts the part of the text generated by the model, excluding the prompt."""
    if full_text.startswith(prompt_text):
        return full_text[len(prompt_text):]
    # Fallback or warning if prompt not found as prefix (should ideally not happen with most HF pipelines)
    print(f"Warning: Prompt not found as a prefix in the generated text. Full text: '{full_text[:100]}...', Prompt: '{prompt_text[:100]}...'")
    return full_text # Or raise an error, or return a specific marker


def _best_of_k(outputs: List[Dict[str, str]], true_val: int, prompts_for_outputs: List[str]) -> Optional[int]:
    """
    Return the integer prediction closest to *true_val* among *outputs*.
    Extracts generated part using corresponding prompt.
    """
    if len(outputs) != len(prompts_for_outputs):
        # This case should ideally be prevented by how outputs and prompts are paired upstream.
        print(f"Warning: Mismatch between number of outputs ({len(outputs)}) and prompts ({len(prompts_for_outputs)}) in _best_of_k.")
        # Fallback: try to parse all outputs directly, though this might include prompt text if not handled.
        # A more robust solution would be to ensure they always match or handle this error more gracefully.
        preds = [_parse_int(o["generated_text"]) for o in outputs]
    else:
        generated_parts = [_extract_generated_part(o["generated_text"], p) for o, p in zip(outputs, prompts_for_outputs)]
        preds = [_parse_int(gp) for gp in generated_parts]
        
    preds = [p for p in preds if p is not None and p in [0, 1]] # Filter for valid boolean (0/1) outputs
    if not preds:
        return None

    # For boolean, if true_val is not present, any valid pred is as good as another if we don't have a distance metric.
    # However, the original logic was "closest". For 0/1, this doesn't make much sense if not exact.
    # Let's return the most common valid prediction, or the first one if tied.
    if preds:
        # Count occurrences of 0 and 1
        count_0 = preds.count(0)
        count_1 = preds.count(1)
        if count_0 > count_1: return 0
        if count_1 > count_0: return 1
        return preds[0] # Tie-break by returning the first valid one
    return None


def make_counterfactual_pair_boolean(
    seq_len: int, divergence_index: int, rng: np.random.RandomState=None
) -> Tuple[str, str, List[int], List[int]]:
    """
    Produce two Python programs operating on boolean variables 'x' and 'y'.
    The programs diverge at `divergence_index` but have the same token length.
    Returns program_a, program_b, intermediates_a, intermediates_b.
    Intermediates are the integer value (0 or 1) of 'x' after each operation modifying 'x'.
    Uses a provided rng for reproducibility.
    """
    if rng is None:
        rng = np.random.RandomState()
    ops = list(GROUPS.keys())

    initial_x = rng.choice([True, False])
    initial_y = rng.choice([True, False])

    program_a_lines = [f"x = {initial_x}", f"y = {initial_y}"]
    program_b_lines = [f"x = {initial_x}", f"y = {initial_y}"]
    
    intermediates_a: List[int] = []
    intermediates_b: List[int] = []

    x_val_a, y_val_a = initial_x, initial_y
    x_val_b, y_val_b = initial_x, initial_y

    for i in range(seq_len):
        current_x_a_before_op = x_val_a
        current_y_a_before_op = y_val_a
        current_x_b_before_op = x_val_b
        current_y_b_before_op = y_val_b
        
        if i < divergence_index:
            op_name = rng.choice(ops)
            op_func, arity = GROUPS[op_name]
            
            line = ""
            if arity == 1:
                line = f"x = not x"
                x_val_a = op_func(current_x_a_before_op)
            else:
                line = f"x = x {op_name} y"
                x_val_a = op_func(current_x_a_before_op, current_y_a_before_op)
            
            program_a_lines.append(line)
            intermediates_a.append(int(x_val_a))
            
            program_b_lines.append(line)
            x_val_b = x_val_a
            intermediates_b.append(int(x_val_b))
        
        elif i == divergence_index:
            suffix_rng_state = rng.get_state() # Save for mirrored suffix

            # Branch A divergence: Use a temporary RNG derived from main rng for this choice
            # to not disturb sequence for B's choice attempt or suffix.
            # However, original _02 used the main rng and let it continue.
            # Let's follow _02 structure for divergence generation more closely.
            # Original logic: use current `rng` for A, then try to make B different using a copy of `rng`'s state.
            
            # --- Branch A divergence ---
            # `rng` state will be used for A's op, then for suffix.
            op_name_a = rng.choice(ops)
            op_func_a, arity_a = GROUPS[op_name_a]
            line_a = ""
            if arity_a == 1:
                line_a = f"x = not x"
                x_val_a = op_func_a(current_x_a_before_op)
            else:
                line_a = f"x = x {op_name_a} y"
                x_val_a = op_func_a(current_x_a_before_op, current_y_a_before_op)
            program_a_lines.append(line_a)
            intermediates_a.append(int(x_val_a))

            # --- Branch B divergence ---
            # Use a *copy* of the rng state *before* A's divergent op was chosen.
            # This means suffix_rng_state is actually state *after* A's choice.
            # No, the _02 logic was:
            # state = rng.get_state() # State *before* A's divergent choice
            # op_a = rng.choice() -> rng advances
            # ...
            # rng_b.set_state(state) -> rng_b starts from where A started its choice
            # op_b = rng_b.choice()
            # ...
            # rng = rng_a (which is the main rng that chose op_a and will do suffix)
            # This is a bit complex. A simpler way for divergence:
            # 1. Choose op_a using main rng.
            # 2. Try to choose op_b using main rng (or a copy) that results in a different *line*.
            
            rng_for_b_op = np.random.RandomState() # Create a new RNG for B's choice attempt
            rng_for_b_op.set_state(suffix_rng_state) # Start B's choice from where A's choice was made
                                                 # No, suffix_rng_state is *after* A's op was chosen by main rng.
                                                 # This is what we want for suffix.
                                                 # For B's divergent op, it needs to be based on state *before* A's op.

            # Let's use the logic from _02_opensource_entity_tracking more directly for divergence:
            # state_before_a_divergence_choice = rng.get_state() #This is what we'd need if rng drives suffix directly
            # op_a = rng.choice(ops) ...
            # rng_b = RandomState(); rng_b.set_state(state_before_a_divergence_choice)
            # op_b = rng_b.choice(ops)
            # while op_b_line == op_a_line: op_b = rng_b.choice(ops)
            # rng.set_state(rng_for_a_op_state_after_choice) # To ensure suffix is based on A's continuation

            # Simpler: Use distinct RNG for B's choice to ensure it can differ, main `rng` continues for suffix
            # `rng` has already made a choice for A's op at divergence.
            # Now for B's op at divergence:
            rng_b_choice = np.random.RandomState(rng.randint(0, 2**32-1)) # Independent RNG for B's choice

            line_b = ""
            chosen_op_name_b = ""
            while True:
                op_name_b_candidate = rng_b_choice.choice(ops)
                op_func_b, arity_b = GROUPS[op_name_b_candidate]
                
                temp_line_b = ""
                if arity_b == 1:
                    temp_line_b = f"x = not x"
                else:
                    temp_line_b = f"x = x {op_name_b_candidate} y"

                if temp_line_b != line_a:
                    line_b = temp_line_b
                    chosen_op_name_b = op_name_b_candidate
                    if arity_b == 1:
                        x_val_b = op_func_b(current_x_b_before_op)
                    else:
                        x_val_b = op_func_b(current_x_b_before_op, current_y_b_before_op)
                    break
                # If only one type of op structure (e.g. only "not x"), this loop might be infinite.
                # This shouldn't happen with current GROUPS. Check len(ops) or structure variability if issues.
                if len(set(f"x = not x" if GROUPS[o][1]==1 else f"x = x {o} y" for o in ops)) == 1 and temp_line_b == line_a:
                    # Only one possible line structure, so divergence in text is impossible.
                    # This is an edge case. For now, assume GROUPS allows textual divergence.
                    line_b = temp_line_b # Fallback, though not ideal for "divergence"
                    chosen_op_name_b = op_name_b_candidate
                    if arity_b == 1: x_val_b = op_func_b(current_x_b_before_op)
                    else: x_val_b = op_func_b(current_x_b_before_op, current_y_b_before_op)
                    # print(f"Warning: Could not create textually divergent line for B. A: '{line_a}', B: '{line_b}'")
                    break


            program_b_lines.append(line_b)
            intermediates_b.append(int(x_val_b))
            
            # `rng` continues from its state after choosing op_a, for mirrored suffix.
            # (No explicit reset needed if rng for B was separate)

        else: # i > divergence_index: Mirrored suffix
            # Suffix operations use the main `rng` which continued from A's divergent choice path
            op_name = rng.choice(ops) 
            op_func, arity = GROUPS[op_name]
            
            line = ""
            if arity == 1:
                line = f"x = not x"
                x_val_a = op_func(x_val_a)
                x_val_b = op_func(x_val_b)
            else:
                line = f"x = x {op_name} y"
                x_val_a = op_func(x_val_a, y_val_a)
                x_val_b = op_func(x_val_b, y_val_b)

            program_a_lines.append(line)
            intermediates_a.append(int(x_val_a))
            program_b_lines.append(line)
            intermediates_b.append(int(x_val_b))

    return "\n".join(program_a_lines), "\n".join(program_b_lines), intermediates_a, intermediates_b

#%% --- Interactive Test Cell ---
# This cell is for quick testing of the generation and model invocation.
# Make sure to run the cells above this one first to define functions and load a model (if testing llm).

# # To test make_counterfactual_pair_boolean:
# if __name__ == '__main__':
#     model_id_test = DEFAULT_MODELS[0] 
#     llm_test = pipeline("text-generation", model=model_id_test, device_map="auto", torch_dtype=torch.bfloat16)

# #%%
# if __name__ == '__main__': # Protect against running automatically if imported
#     # print("--- Testing make_counterfactual_pair_boolean ---")
#     # test_rng = np.random.RandomState(42) # For reproducible test
#     test_seq_len = 5
#     test_divergence_index = 1
    
#     prog_a, prog_b, intermediates_a, intermediates_b = make_counterfactual_pair_boolean(
#         test_seq_len, test_divergence_index
#     )
#     # print("Program A:")
#     # print(prog_a)
#     # print(f"Intermediates A: {intermediates_a} (Final x: {intermediates_a[-1] if intermediates_a else 'N/A'})")
#     # print("\nProgram B:")
#     # print(prog_b)
#     # print(f"Intermediates B: {intermediates_b} (Final x: {intermediates_b[-1] if intermediates_b else 'N/A'})")
#     # print("--------------------------------------------")

#     # To test with an LLM (ensure a model is loaded in a cell above)
    
#     if 'llm_test' in locals() and prog_a and intermediates_a:
#         true_val_a = intermediates_a[-1]
#         prompt_a = PROMPT_TEMPLATE.format(code=prog_a)
#         print(f"\n--- Testing LLM with Program A (True value: {true_val_a}) ---")
#         # print(f"Prompt A: {prompt_a}")
        
#         llm_outputs = llm_test([prompt_a], num_return_sequences=1, max_new_tokens=1, do_sample=False)
#         generated_text_a = llm_outputs[0][0]['generated_text'] # Assuming batch_size=1, num_return_sequences=1
#         pred_a = _parse_int(generated_text_a[len(prompt_a):])
        
#         print(f"LLM Raw Output A: {generated_text_a}" + "\n" + "--------------------------------")
#         print(f"LLM Parsed Prediction A: {pred_a}" )
#         print(f"Ground truth A: {true_val_a}" + "\n" + "--------------------------------")
#         print(f"Correct A: {pred_a == true_val_a if pred_a is not None else 'Parse Error'}" + "\n" + "--------------------------------")
#     # else:
#     # print("\nLLM test part skipped (llm_test not defined or program generation failed).")
#     # print("Ensure you have a 'llm_test' pipeline object initialized to run the LLM test part.")
#     pass # End of if __name__ == '__main__' block for interactive testing

#%%
@click.command()
@click.option("--num-seqs", default=100, help="Number of counterfactual pairs per model.")
@click.option("--seq-len", "seq_len_opt", default=5, help="Number of operations in each generated program.")
@click.option("--best-of", "best_of_k_samples", default=1, help="Number of samples per prompt (set to 1 for one-shot).")
@click.option("--output-dir", "output_base_dir_str", required=True, type=click.Path(file_okay=False, dir_okay=True), help="Base directory to save all results (timestamped dir created by calling script).")
@click.option("--models", "model_ids_str", default=None, help="Comma-separated list of model IDs to run. Overrides DEFAULT_MODELS/DEFAULT_MODELS_ALL.")
def main(num_seqs: int, seq_len_opt: int, best_of_k_samples: int, output_base_dir_str: str, model_ids_str: Optional[str]):
    """Evaluate models on the boolean variable tracking task."""
    accelerator = Accelerator()

    # output_base_dir is the same for all processes
    output_base_dir = Path(output_base_dir_str)
    # Detailed data for this specific seq_len will go into a subdirectory (same for all processes)
    detailed_results_dir_for_seq_len = output_base_dir / f"detailed_data_len{seq_len_opt}"
    if accelerator.is_main_process: # Create directories only on main process
        detailed_results_dir_for_seq_len.mkdir(parents=True, exist_ok=True)
    
    # Global summary CSV in the base output directory (same for all processes)
    global_summary_csv_path = output_base_dir / "summary_boolean_all.csv"
    
    # Determine models to run (all processes do this to have the full list)
    all_models_to_consider: List[str]
    if model_ids_str:
        all_models_to_consider = [m.strip() for m in model_ids_str.split(',')]
        if accelerator.is_main_process:
            print(f"[Process {accelerator.process_index}] Running with models specified via --models: {all_models_to_consider}")
    else:
        all_models_to_consider = DEFAULT_MODELS_ALL # Changed to use DEFAULT_MODELS_ALL by default if not specified
        if accelerator.is_main_process:
            print(f"[Process {accelerator.process_index}] Running with default models (DEFAULT_MODELS_ALL): {all_models_to_consider}")

    # Distribute models among processes
    models_to_run_this_process = all_models_to_consider[accelerator.process_index::accelerator.num_processes]
    
    if not models_to_run_this_process:
        if accelerator.is_main_process or accelerator.num_processes == 1 : # Avoid spamming if many processes get no models
             print(f"[Process {accelerator.process_index}] No models assigned to this process. Exiting.")
        return # Important to exit if no models for this process

    print(f"[Process {accelerator.process_index}/{accelerator.num_processes}] Assigned models: {models_to_run_this_process}")


    # Read existing results from the global summary to skip already evaluated (model, seq_len) pairs
    # All processes read, but only main process writes header.
    processed_model_seq_len_pairs = set()
    if global_summary_csv_path.exists(): # Check existence before trying to read
        try:
            # Ensure read is consistent if multiple processes are checking
            # For simplicity, we assume eventual consistency or that main process creates it first.
            # A lock could be used here if concurrent read/writes become an issue for initial creation.
            with global_summary_csv_path.open("r", newline="") as f:
                reader = csv.DictReader(f)
                for row in reader:
                    if row and "model_id" in row and "seq_len" in row:
                        try:
                            processed_model_seq_len_pairs.add((row["model_id"], int(row["seq_len"])))
                        except ValueError:
                            if accelerator.is_main_process: # Print warning only once
                                print(f"Warning: Could not parse seq_len from row in summary: {row}")
        except Exception as e:
            if accelerator.is_main_process: # Print warning only once
                print(f"Warning: Could not read existing global results from {global_summary_csv_path}: {e}")

    # Check if the global summary CSV needs a header (only main process will write it)
    needs_global_header = not global_summary_csv_path.exists() or global_summary_csv_path.stat().st_size == 0
    if accelerator.is_main_process and needs_global_header:
        try:
            with global_summary_csv_path.open("w", newline="") as f: # Use "w" to create/truncate
                writer = csv.writer(f)
                writer.writerow(["model_id", "seq_len", "accuracy_A", "accuracy_B", "accuracy_avg_pair", "num_pairs_evaluated", "num_seqs_requested", "best_of_k"])
            needs_global_header = False # Header is now written by main process
            print(f"[Process {accelerator.process_index}] Global summary CSV header written to {global_summary_csv_path}")
        except IOError as e:
            print(f"[Process {accelerator.process_index}] ERROR: Could not write header to {global_summary_csv_path}: {e}")
            # Potentially exit or raise if header writing is critical and fails
    
    accelerator.wait_for_everyone() # Ensure header is written before other processes might try to append

    master_rng = np.random.RandomState(12345) # Fixed seed for the entire experiment run

    for model_id in models_to_run_this_process: # Iterate over subset of models for this process
        if (model_id, seq_len_opt) in processed_model_seq_len_pairs:
            print(f"[Process {accelerator.process_index}] Skipping model {model_id} for seq_len {seq_len_opt} as it's already in {global_summary_csv_path}.")
            continue

        print(f"\n[Process {accelerator.process_index}] --- Processing Model: {model_id}, Sequence Length: {seq_len_opt} ---")
        llm = None
        try:
            pipeline_init_kwargs = {
                "trust_remote_code": True,
                # device_map="auto" works with Accelerator. Each process gets its own pipeline.
                # Accelerator will manage GPU assignment for each process.
                "device_map": accelerator.device, # Use device from Accelerator
                "torch_dtype": torch.bfloat16,
            }
            # ... (FlashAttention logic can remain, applied per process)
            # if torch.cuda.is_available():
            #     major_capability, _ = torch.cuda.get_device_capability(accelerator.device)
            #     if major_capability >= 8: 
            #         if "qwen" in model_id.lower():
            #             pipeline_init_kwargs.setdefault("model_kwargs", {})["attn_implementation"] = "flash_attention_2"
            #             # print(f"[Process {accelerator.process_index}] INFO: Attempting to set 'flash_attention_2' for model {model_id}.")
            
            tok = AutoTokenizer.from_pretrained(model_id, padding_side="left")   
            llm = pipeline("text-generation", model=model_id, tokenizer=tok, **pipeline_init_kwargs)
            
            print(f"[Process {accelerator.process_index}] Model {model_id} loaded on {llm.device}. Effective model dtype: {llm.model.dtype if llm and hasattr(llm, 'model') else 'N/A'}.")
            if llm and hasattr(llm, 'model') and hasattr(llm.model.config, '_attn_implementation'):
                 print(f"[Process {accelerator.process_index}] Model {model_id} configured with attention implementation: {llm.model.config._attn_implementation}")

        except Exception as e:
            print(f"[Process {accelerator.process_index}] ERROR: Could not load model {model_id}: {e}. Skipping model.")
            if llm: del llm
            if torch.cuda.is_available(): torch.cuda.empty_cache()
            gc.collect()
            continue
            
        correct_a, correct_b = 0, 0
        total_evaluated_pairs = 0
        all_model_data = [] # Specific to this model, this process

        for i_seq in range(num_seqs):
            # Divergence index
            divergence_idx = max(0, min(seq_len_opt - 1, seq_len_opt // 2))

            # Generate programs (master_rng might need to be managed if different processes need distinct sequences for the *same* model,
            # but here models are split, so each process works on different models, making master_rng usage for sequence generation fine)
            prog_a, prog_b, intermediates_a, intermediates_b = make_counterfactual_pair_boolean(
                seq_len_opt, divergence_idx, master_rng 
            )

            if not intermediates_a or not intermediates_b:
                print(f"  [Process {accelerator.process_index}] WARNING: make_counterfactual_pair_boolean returned empty intermediates for model {model_id}, seq {i_seq+1}. Skipping.")
                continue

            true_val_a = intermediates_a[-1]
            true_val_b = intermediates_b[-1]
            
            prompt_text_a = PROMPT_TEMPLATE.format(code=prog_a)
            prompt_text_b = PROMPT_TEMPLATE.format(code=prog_b)
            
            prompts_batch = [prompt_text_a, prompt_text_b]
            # true_vals_batch = [true_val_a, true_val_b] # Already have true_val_a, true_val_b
            # codes_batch = [prog_a, prog_b] # Already have prog_a, prog_b
            # intermediates_full_batch = [intermediates_a, intermediates_b] # Already have these

            current_pair_data = {
                "pair_id": i_seq,
                "seq_len": seq_len_opt,
                "divergence_index": divergence_idx,
                "program_a": prog_a, "intermediates_a": intermediates_a, "true_val_a": true_val_a,
                "program_b": prog_b, "intermediates_b": intermediates_b, "true_val_b": true_val_b,
                "pred_a": None, "pred_b": None, "correct_a": False, "correct_b": False,
                "outputs_a": None, "outputs_b": None
            }

            try:
                # batch_size for pipeline with device_map="auto" or accelerator.device
                # The pipeline will handle batching internally if multiple prompts are given.
                # Here, we are processing one pair (2 prompts) at a time.
                pipeline_outputs = llm(
                    prompts_batch,
                    num_return_sequences=best_of_k_samples, # Should be 1 based on run script
                    max_new_tokens=10, 
                    do_sample=True if best_of_k_samples > 1 else False,
                    temperature=0.8 if best_of_k_samples > 1 else 0.0,
                    batch_size=len(prompts_batch) 
                )
            except Exception as e:
                print(f"  [Process {accelerator.process_index}] ERROR: Runtime error during LLM inference for model {model_id}, seq {i_seq+1}: {e}. Skipping pair.")
                if torch.cuda.is_available(): torch.cuda.empty_cache()
                gc.collect()
                continue 

            if len(pipeline_outputs) != len(prompts_batch):
                print(f"  [Process {accelerator.process_index}] WARNING: LLM output length mismatch for model {model_id}, seq {i_seq+1}. Expected {len(prompts_batch)}, got {len(pipeline_outputs)}. Skipping pair.")
                continue

            outputs_a_list = pipeline_outputs[0]
            outputs_b_list = pipeline_outputs[1]

            pred_a = _best_of_k(outputs_a_list, true_val_a, [prompt_text_a] * len(outputs_a_list))
            current_pair_data["outputs_a"] = outputs_a_list
            current_pair_data["pred_a"] = pred_a
            if pred_a is not None and pred_a == true_val_a:
                correct_a += 1
                current_pair_data["correct_a"] = True

            pred_b = _best_of_k(outputs_b_list, true_val_b, [prompt_text_b] * len(outputs_b_list))
            current_pair_data["outputs_b"] = outputs_b_list
            current_pair_data["pred_b"] = pred_b
            if pred_b is not None and pred_b == true_val_b:
                correct_b += 1
                current_pair_data["correct_b"] = True
            
            all_model_data.append(current_pair_data)
            total_evaluated_pairs += 1
            
            if (i_seq + 1) % 20 == 0: # Log progress less frequently for multi-process
                 print(f"  [P{accelerator.process_index}][M {model_id}, SL {seq_len_opt}] Pair {i_seq+1}/{num_seqs}. Acc A: {correct_a/(i_seq+1):.2f}, Acc B: {correct_b/(i_seq+1):.2f}")


        if total_evaluated_pairs > 0:
            acc_a = correct_a / total_evaluated_pairs
            acc_b = correct_b / total_evaluated_pairs
            avg_acc = (correct_a + correct_b) / (total_evaluated_pairs * 2) if total_evaluated_pairs > 0 else 0.0

            print(f"[Process {accelerator.process_index}] Model {model_id} (SeqLen {seq_len_opt}): Overall Acc A: {acc_a:.2%}, Acc B: {acc_b:.2%}, Avg Pair Acc: {avg_acc:.2%}")
            
            model_name_safe = model_id.replace('/', '_')
            # Detailed path uses detailed_results_dir_for_seq_len which is common, but filename is unique per model (and models are split)
            detailed_path = detailed_results_dir_for_seq_len / f"{model_name_safe}_boolean_data.json"
            with detailed_path.open("w") as f: # Each process writes its own model's detailed file
                json.dump(all_model_data, f, indent=2)
            print(f"[Process {accelerator.process_index}] Detailed results for {model_id} (SeqLen {seq_len_opt}) saved to {detailed_path}")

            # Append to the global summary CSV (potential for concurrent appends from different processes for different models)
            # This should be mostly safe as they are appending different rows. File locking could make it safer.
            # For now, rely on OS to handle append operations from multiple processes.
            # The header is written by the main process only, and other processes wait.
            try:
                with global_summary_csv_path.open("a", newline="") as f:
                    writer = csv.writer(f)
                    # Header should already be written by main process, or this is not main process
                    writer.writerow([model_id, seq_len_opt, f"{acc_a:.6f}", f"{acc_b:.6f}", f"{avg_acc:.6f}", total_evaluated_pairs, num_seqs, best_of_k_samples])
                print(f"[Process {accelerator.process_index}] Global summary for {model_id} (SeqLen {seq_len_opt}) appended to {global_summary_csv_path}")
            except IOError as e:
                 print(f"[Process {accelerator.process_index}] ERROR: Could not append to global summary {global_summary_csv_path} for model {model_id}: {e}")
        else:
            print(f"[Process {accelerator.process_index}] No pairs were successfully evaluated for model {model_id} (SeqLen {seq_len_opt}). Attempting to record this.")
            if not (model_id, seq_len_opt) in processed_model_seq_len_pairs:
                try:
                    with global_summary_csv_path.open("a", newline="") as f:
                        writer = csv.writer(f)
                        # Header logic: if needs_global_header is True here AND this is main process, it would have been written.
                        # If not main process, assume header exists or main process handles it.
                        # This edge case (first non-main process to write if main had no models) needs careful thought
                        # if not global_summary_csv_path.exists() or global_summary_csv_path.stat().st_size == 0:
                        # This re-check for header is problematic in multi-process. Relies on main process creating it.
                        writer.writerow([model_id, seq_len_opt, "0.000000", "0.000000", "0.000000", 0, num_seqs, best_of_k_samples])
                    print(f"[Process {accelerator.process_index}] Recorded 0 evaluation for {model_id} (SeqLen {seq_len_opt}) in {global_summary_csv_path}")
                except IOError as e:
                    print(f"[Process {accelerator.process_index}] ERROR: Could not append 0-eval to global summary {global_summary_csv_path} for model {model_id}: {e}")


        if llm: del llm
        if torch.cuda.is_available(): torch.cuda.empty_cache()
        gc.collect()
        print(f"[Process {accelerator.process_index}] --- Finished Model: {model_id} ---")

    accelerator.wait_for_everyone() # Ensure all processes finish before the script ends for this seq_len
    if accelerator.is_main_process:
        print(f"\n[Process {accelerator.process_index}] All assigned models processed for sequence length {seq_len_opt}.")
        print(f"Summary results are in {global_summary_csv_path}")

if __name__ == "__main__":
    main()


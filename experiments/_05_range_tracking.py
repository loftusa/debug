#%%
import re
from pathlib import Path
from typing import List, Optional, Dict

import click
from transformers import pipeline, AutoTokenizer
import csv
import json
import gc
import torch
import numpy as np
from accelerate import Accelerator

print('imports loaded')


PROMPT_TEMPLATE = (
    "You are given a short Python program. "
    "Your task is to compute the final value of the variable x. "
    "Return only the integer, without commas, an equal sign, or any additional text. The integer should appear immediately after the word 'is: '.\n" 
    "```python\n{code}\n```\n"
    "The final value of x is: "
)


DEFAULT_MODELS_ALL: List[str] = [
    "Qwen/Qwen2.5-0.5B",
    "Qwen/Qwen2.5-1.5B",
    "Qwen/Qwen2.5-3B",
    "Qwen/Qwen2.5-7B",
    "Qwen/Qwen2.5-14B",
    # "Qwen/Qwen2.5-32B",
]


def _parse_int(text: str) -> Optional[int]:
    """Extract the integer following 'the final value of x is: ' (case-insensitive, commas ignored), or the first integer otherwise."""
    cleaned = text.replace(",", "")
    # Try to find integer after the explicit phrase
    phrase_match = re.search(r"is:\s*(-?\d+)", cleaned, flags=re.IGNORECASE)
    if phrase_match:
        return int(phrase_match.group(1))
    # Fallback: first integer anywhere
    match = re.search(r"-?\d+", cleaned)
    return int(match.group()) if match else None


def _extract_generated_part(full_text: str, prompt_text: str) -> str:
    """Extracts the part of the text generated by the model, excluding the prompt."""
    if full_text.startswith(prompt_text):
        return full_text[len(prompt_text):]
    return full_text


def make_range_program(seq_len: int, rng: np.random.RandomState) -> tuple[str, int]:
    """
    Generate a simple range tracking program:
    x = 0
    for i in range(n):
        x += z
    
    Returns program text and true final value (n * z).
    """
    z = rng.randint(1, 10)  # Random increment value
    
    program = f"""x = 0
for i in range({seq_len}):
    x += {z}"""
    
    true_value = seq_len * z
    return program, true_value

def make_range_program_newlines(seq_len: int, rng: np.random.RandomState, random_sum=False) -> tuple[str, int]:
    """
    Generate a simple range tracking program with explicit lines:
    x = 0
    x += z
    x += z
    ...
    
    Returns program text and true final value.
    """
    lines = ["x = 0"]
    total_sum = 0
    
    for _ in range(seq_len):
        z = rng.randint(1, 10)  # Random increment value for each line
        lines.append(f"x += {z}")
        total_sum += z
        
        # If not random_sum, keep using the same z for all lines
        if not random_sum:
            # For non-random case, generate all lines with the same z
            break
    
    if not random_sum:
        # Generate remaining lines with the same z value
        for _ in range(1, seq_len):
            lines.append(f"x += {z}")
            total_sum += z
    
    program = "\n".join(lines)
    true_value = total_sum
    return program, true_value

@click.command()
@click.option("--num-seqs", default=100, help="Number of programs per model.")
@click.option("--seq-len", "seq_len_opt", default=5, help="Number of iterations in the range loop.")
@click.option("--best-of", "best_of_k_samples", default=1, help="Number of samples per prompt.")
@click.option("--output-dir", "output_base_dir_str", required=True, type=click.Path(file_okay=False, dir_okay=True), help="Base directory to save all results.")
@click.option("--models", "model_ids_str", default=None, help="Comma-separated list of model IDs to run.")
@click.option("--program-type", "program_type", default="lines", help="Type of program to generate: 'lines' or 'single'.")
@click.option("--random-sum", "random_sum", default=False, help="Whether to use random sums in the program.")
def main(num_seqs: int, seq_len_opt: int, best_of_k_samples: int, output_base_dir_str: str, model_ids_str: Optional[str], program_type: str, random_sum: bool):
    """Evaluate models on the simple range tracking task."""
    accelerator = Accelerator()

    output_base_dir = Path(output_base_dir_str)
    detailed_results_dir = output_base_dir / f"detailed_data_len{seq_len_opt}"
    if accelerator.is_main_process:
        detailed_results_dir.mkdir(parents=True, exist_ok=True)
    
    global_summary_csv_path = output_base_dir / "summary_range_tracking.csv"
    
    if model_ids_str:
        all_models_to_consider = [m.strip() for m in model_ids_str.split(',')]
    else:
        all_models_to_consider = DEFAULT_MODELS_ALL

    models_to_run_this_process = all_models_to_consider[accelerator.process_index::accelerator.num_processes]
    
    if not models_to_run_this_process:
        print(f"[Process {accelerator.process_index}] No models assigned to this process. Exiting.")
        return

    print(f"[Process {accelerator.process_index}/{accelerator.num_processes}] Assigned models: {models_to_run_this_process}")

    # Check for existing results
    processed_models = set()
    if global_summary_csv_path.exists():
        try:
            with global_summary_csv_path.open("r") as f:
                reader = csv.DictReader(f)
                for row in reader:
                    if row and "model_id" in row and "seq_len" in row:
                        try:
                            processed_models.add((row["model_id"], int(row["seq_len"])))
                        except ValueError:
                            pass
        except Exception as e:
            if accelerator.is_main_process:
                print(f"Warning: Could not read existing results: {e}")

    # Write header if needed
    needs_header = not global_summary_csv_path.exists() or global_summary_csv_path.stat().st_size == 0
    if accelerator.is_main_process and needs_header:
        with global_summary_csv_path.open("w", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["model_id", "seq_len", "accuracy", "num_evaluated", "num_requested", "best_of_k"])
    
    accelerator.wait_for_everyone()

    master_rng = np.random.RandomState(12345)

    for model_id in models_to_run_this_process:
        if (model_id, seq_len_opt) in processed_models:
            print(f"[Process {accelerator.process_index}] Skipping {model_id} for seq_len {seq_len_opt} - already processed.")
            continue

        print(f"[Process {accelerator.process_index}] Processing {model_id}, seq_len {seq_len_opt}")
        
        try:
            tok = AutoTokenizer.from_pretrained(model_id, padding_side="left")
            llm = pipeline(
                "text-generation", 
                model=model_id, 
                tokenizer=tok,
                trust_remote_code=True,
                device_map=accelerator.device,
                torch_dtype=torch.bfloat16
            )
            print(f"[Process {accelerator.process_index}] Model {model_id} loaded.")
        except Exception as e:
            print(f"[Process {accelerator.process_index}] ERROR loading {model_id}: {e}")
            continue
            
        correct = 0
        total_evaluated = 0
        all_results = []

        for i_seq in range(num_seqs):
            if program_type == "lines":
                program, true_value = make_range_program_newlines(seq_len_opt, master_rng, random_sum)
            else:
                program, true_value = make_range_program(seq_len_opt, master_rng)
            prompt_text = PROMPT_TEMPLATE.format(code=program)
            
            result_data = {
                "seq_id": i_seq,
                "seq_len": seq_len_opt,
                "program": program,
                "true_value": true_value,
                "predicted_value": None,
                "correct": False,
                "raw_outputs": None
            }

            try:
                outputs = llm(
                    prompt_text,
                    num_return_sequences=best_of_k_samples,
                    max_new_tokens=10,
                    do_sample=True if best_of_k_samples > 1 else False,
                    temperature=0.8 if best_of_k_samples > 1 else 0.0
                )
                
                result_data["raw_outputs"] = outputs
                
                # Parse predictions and find best one
                predictions = []
                for output in outputs:
                    generated_part = _extract_generated_part(output["generated_text"], prompt_text)
                    pred = _parse_int(generated_part)
                    if pred is not None:
                        predictions.append(pred)
                
                if predictions:
                    # If any prediction is exactly correct, use it
                    if true_value in predictions:
                        best_pred = true_value
                    else:
                        # Otherwise, use the closest one
                        best_pred = min(predictions, key=lambda p: abs(p - true_value))
                    
                    result_data["predicted_value"] = best_pred
                    if best_pred == true_value:
                        correct += 1
                        result_data["correct"] = True
                
                total_evaluated += 1
                
            except Exception as e:
                print(f"  ERROR in inference for seq {i_seq}: {e}")
                
            all_results.append(result_data)
            
            if (i_seq + 1) % 20 == 0:
                acc = correct / total_evaluated if total_evaluated > 0 else 0
                print(f"  Seq {i_seq+1}/{num_seqs}, Accuracy: {acc:.2%}")

        # Save results
        if total_evaluated > 0:
            accuracy = correct / total_evaluated
            print(f"[Process {accelerator.process_index}] {model_id}: Accuracy {accuracy:.2%} ({correct}/{total_evaluated})")
            
            # Save detailed results
            model_name_safe = model_id.replace('/', '_')
            detailed_path = detailed_results_dir / f"{model_name_safe}_range_tracking.json"
            with detailed_path.open("w") as f:
                json.dump(all_results, f, indent=2)

            # Update summary
            with global_summary_csv_path.open("a", newline="") as f:
                writer = csv.writer(f)
                writer.writerow([model_id, seq_len_opt, f"{accuracy:.6f}", total_evaluated, num_seqs, best_of_k_samples])
        
        # Cleanup
        del llm
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()

    accelerator.wait_for_everyone()
    if accelerator.is_main_process:
        print(f"All models processed. Results in {global_summary_csv_path}")


if __name__ == "__main__":
    main()

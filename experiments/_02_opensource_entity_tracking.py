#%%
from ast import Import
import re
from pathlib import Path
import struct
from typing import List, Tuple, Optional, Dict

import click
from transformers import pipeline, AutoTokenizer
import csv
import json
import gc
import torch
import numpy as np
from accelerate import Accelerator

print('imports loaded')


PROMPT_TEMPLATE = (
    "You are given a short Python program. "
    "Your task is to compute the final value of the variable x. "
    "Return only the integer, without commas, an equal sign, or any additional text. The integer should appear immediately after the word 'is: '.\n" 
    "```python\n{code}\n```\n"
    "The final value of x is: "
)


DEFAULT_MODELS_ALL: List[str] = [
    "Qwen/Qwen3-0.6B",
    "Qwen/Qwen3-1.7B",
    "Qwen/Qwen3-4B",
    "Qwen/Qwen3-8B",
    "Qwen/Qwen3-14B",
    # "Qwen/Qwen3-32B",
]


def _parse_int(text: str) -> Optional[int]:
    """Extract the integer following 'the final value of x is: ' (case-insensitive, commas ignored), or the first integer otherwise."""
    cleaned = text.replace(",", "")
    # Try to find integer after the explicit phrase
    phrase_match = re.search(r"is:\s*(-?\d+)", cleaned, flags=re.IGNORECASE)
    if phrase_match:
        return int(phrase_match.group(1))
    # Fallback: first integer anywhere
    match = re.search(r"-?\d+", cleaned)
    return int(match.group()) if match else None


def _extract_generated_part(full_text: str, prompt_text: str) -> str:
    """Extracts the part of the text generated by the model, excluding the prompt."""
    if full_text.startswith(prompt_text):
        return full_text[len(prompt_text):]
    # Fallback or warning if prompt not found as prefix (should ideally not happen with most HF pipelines)
    print(f"Warning: Prompt not found as a prefix in the generated text. Full text: '{full_text[:100]}...', Prompt: '{prompt_text[:100]}...'")
    return full_text


## DEBUG
# model_id = "Qwen/Qwen2.5-Coder-32B-Instruct"
# tok = AutoTokenizer.from_pretrained(model_id)
# llm = pipeline("text-generation", model=model_id, tokenizer=tok, temperature=0.8, trust_remote_code=True, device_map="auto", torch_dtype=torch.bfloat16)
#%%
# --- Start of interactive test cell ---
# Configuration for the test
# test_seq_len = 3
# test_divergence_index = 2
# best_of_k_test = 1 # Number of samples for _best_of_k

# Generate a single code sequence and its true value

GROUPS = {
    "add": ("+= v", lambda x, v: x + v),
    "sub": ("-= v", lambda x, v: x - v),
    # "mul": ("*= v", lambda x, v: x * v)image.png,
    # "div": ("//= v", lambda x, v: x // v),
    # "mod": ("%= v", lambda x, v: x % v),
    # "pow": ("**= v", lambda x, v: x**v),
    # "abs": ("= abs(x - v)", lambda x, v: abs(x - v)),
}

# OP_TO_GROUP = { # This was not used in the main logic, can be kept or removed. For now, keeping.
#     "add": "additive",
#     "sub": "additive",
#     # "mul": "multiplicative",
#     # "div": "multiplicative",
#     # "mod": "modular",
#     # "pow": "exponential",
#     # "abs": "absdiff",
# }

# Modified to align with _04_boolean_experiment.py's _best_of_k structure
def _best_of_k(outputs: List[Dict[str, str]], true_val: int, prompts_for_outputs: List[str]) -> Optional[int]:
    """
    Return the integer prediction closest to *true_val* among *outputs*.
    Extracts generated part using corresponding prompt.
    """
    if len(outputs) != len(prompts_for_outputs):
        print(f"Warning: Mismatch between number of outputs ({len(outputs)}) and prompts ({len(prompts_for_outputs)}) in _best_of_k.")
        # Fallback: try to parse all outputs directly, though this might include prompt text if not handled.
        preds = [_parse_int(o["generated_text"]) for o in outputs]
    else:
        generated_parts = [_extract_generated_part(o["generated_text"], p) for o, p in zip(outputs, prompts_for_outputs)]
        preds = [_parse_int(gp) for gp in generated_parts]
        
    # Filter Nones
    preds = [p for p in preds if p is not None]
    if not preds:
        return None
    # If any exactly equals, prefer that
    for p in preds:
        if p == true_val:
            return p

    # Otherwise, return closest value to true
    return min(preds, key=lambda p: abs(p - true_val))

def make_counterfactual_pair(
    seq_len: int, divergence_index: int, rng: np.random.RandomState = None # Added rng parameter
) -> Tuple[str, str, List[int], List[int]]:
    """
    Produce two programs of identical token length where they diverge at an early step `k`.
    Returns program_a, program_b, intermediates_a, intermediates_b. The intermediates lists contain the value of x *after* each operation line.
    Uses a provided rng for reproducibility.
    """
    # Initialize RNG for reproducibility
    if rng is None: # Added for optional RNG
        rng = np.random.RandomState()
    # seed = np.random.randint(0, 2**32 - 1) # Original specific seed line removed for passed rng
    # rng = np.random.RandomState(seed)
    ops = list(GROUPS.keys())
    max_v = 5

    # Prepare program lines and intermediates for both branches
    program_a_lines, program_b_lines = ["x = 0"], ["x = 0"] # Renamed for clarity
    # Start intermediates with the initial value 0, corresponding to "x = 0"
    intermediates_a, intermediates_b = [0], [0]
    # Track current x values for each branch
    x_val_a = 0
    x_val_b = 0

    for i in range(seq_len):
        # current_x_a_before_op = x_val_a # These are not strictly needed if we use x_val_a/b directly
        # current_x_b_before_op = x_val_b

        if i < divergence_index:
            # Common prefix
            op_name = rng.choice(ops) # Renamed op to op_name
            v = rng.randint(1, max_v)
            template, func = GROUPS[op_name]
            expr = template.replace("v", str(v))

            # Update programs
            program_a_lines.append(f"x {expr}")
            program_b_lines.append(f"x {expr}")

            # Calculate next value based on current x_val
            x_val_a = func(x_val_a, v)
            x_val_b = x_val_a  # Keep values synchronized during prefix

            # Append the *result* of this step
            intermediates_a.append(x_val_a)
            intermediates_b.append(x_val_b)
        elif i == divergence_index:
            # Save RNG state to mirror the rest of the sequence generation
            # This specific way of managing rng for A and B divergence and suffix can be complex.
            # _04_ uses a simpler approach for B's divergent choice (new RandomState or distinct path).
            # Let's try to simplify while ensuring divergence and mirrored suffix.
            
            state_before_a_divergence_choice = rng.get_state() # For B to try a different choice from same point

            # --- Branch A divergence ---
            op_name_a = rng.choice(ops) # rng advances
            v_a = rng.randint(1, max_v)
            template_a, func_a = GROUPS[op_name_a]
            expr_a = template_a.replace("v", str(v_a))
            program_a_lines.append(f"x {expr_a}")
            x_val_a = func_a(x_val_a, v_a) # Use x_val_a from *before* this op
            intermediates_a.append(x_val_a)

            # --- Branch B divergence ---
            rng_b_choice = np.random.RandomState()
            rng_b_choice.set_state(state_before_a_divergence_choice) # Reset for B's choice

            op_name_b, v_b = op_name_a, v_a
            expr_b = expr_a
            # Ensure true divergence (op or v must differ, leading to different expr)
            while expr_b == expr_a :
                op_name_b = rng_b_choice.choice(ops)
                v_b = rng_b_choice.randint(1, max_v)
                # template_b, func_b = GROUPS[op_name_b] # func_b not needed here, only for calculation
                # expr_b = template_b.replace("v", str(v_b))
                # Check if op_name_b or v_b are different enough.
                # For integer arithmetic, any different op or v typically creates different text.
                # Re-create expr_b with the new op_name_b, v_b
                template_b_candidate, _ = GROUPS[op_name_b]
                expr_b_candidate = template_b_candidate.replace("v", str(v_b))
                if expr_b_candidate != expr_a:
                    expr_b = expr_b_candidate
                    break
                # If GROUPS is very small or max_v is 1, this could loop. Add a safety break or ensure diversity.
                # For current GROUPS and max_v=5, divergence should be easy.

            _, func_b = GROUPS[op_name_b] # Get func_b for calculation
            program_b_lines.append(f"x {expr_b}")
            x_val_b = func_b(x_val_b, v_b) # Use x_val_b from *before* this op
            intermediates_b.append(x_val_b)
            
            # rng (main one) continues from where A left off, for the suffix.
        else: # i > divergence_index: Mirrored suffix
            op_name = rng.choice(ops)
            v = rng.randint(1, max_v) # Original used randint(1, 10), aligning with prefix max_v=5
            template, func = GROUPS[op_name]
            expr = template.replace("v", str(v))

            program_a_lines.append(f"x {expr}")
            x_val_a = func(x_val_a, v)
            intermediates_a.append(x_val_a)

            program_b_lines.append(f"x {expr}")
            x_val_b = func(x_val_b, v)
            intermediates_b.append(x_val_b)

    prog_a_str = "\n".join(program_a_lines)
    prog_b_str = "\n".join(program_b_lines)

    # Return intermediates *after* each operation (excluding the initial x=0 state which is implied)
    # The original [1:] was to skip the initial 0. If we generate `seq_len` operations,
    # we will have `seq_len` intermediate values. The `x=0` line means `len(intermediates)` will be `seq_len + 1`.
    # So `[1:]` gives `seq_len` values. This seems correct.
    return prog_a_str, prog_b_str, intermediates_a[1:], intermediates_b[1:]

# The interactive test cell from _02_... is removed for brevity and because the main function will be the focus.
# If __name__ == "__main__":
# ... (original test cell) ...

#%%
# Updated main function structure based on _04_boolean_experiment.py
@click.command()
@click.option("--num-seqs", default=100, help="Number of counterfactual pairs per model.") # Matches _04_
@click.option("--seq-len", "seq_len_opt", default=5, help="Number of operations in each generated program.") # Matches _04_ (renamed from seq_len)
@click.option("--best-of", "best_of_k_samples", default=1, help="Number of samples per prompt (set to 1 for one-shot).") # Matches _04_ (renamed from best_of)
@click.option("--output-dir", "output_base_dir_str", required=True, type=click.Path(file_okay=False, dir_okay=True), help="Base directory to save all results.") # Matches _04_
@click.option("--models", "model_ids_str", default=None, help="Comma-separated list of model IDs to run. Overrides DEFAULT_MODELS_ALL.") # Matches _04_
def main(num_seqs: int, seq_len_opt: int, best_of_k_samples: int, output_base_dir_str: str, model_ids_str: Optional[str]):
    """Evaluate models on the integer variable tracking task using counterfactual pairs."""
    accelerator = Accelerator()

    output_base_dir = Path(output_base_dir_str)
    detailed_results_dir_for_seq_len = output_base_dir / f"detailed_data_len{seq_len_opt}"
    if accelerator.is_main_process:
        detailed_results_dir_for_seq_len.mkdir(parents=True, exist_ok=True)
    
    # Global summary CSV, named to distinguish from boolean experiment
    global_summary_csv_path = output_base_dir / "summary_integer_all.csv" 
    
    all_models_to_consider: List[str]
    if model_ids_str:
        all_models_to_consider = [m.strip() for m in model_ids_str.split(',')]
        if accelerator.is_main_process:
            print(f"[Process {accelerator.process_index}] Running with models specified via --models: {all_models_to_consider}")
    else:
        all_models_to_consider = DEFAULT_MODELS_ALL
        if accelerator.is_main_process:
            print(f"[Process {accelerator.process_index}] Running with default models (DEFAULT_MODELS_ALL): {all_models_to_consider}")

    models_to_run_this_process = all_models_to_consider[accelerator.process_index::accelerator.num_processes]
    
    if not models_to_run_this_process:
        if accelerator.is_main_process or accelerator.num_processes == 1:
             print(f"[Process {accelerator.process_index}] No models assigned to this process. Exiting.")
        return

    print(f"[Process {accelerator.process_index}/{accelerator.num_processes}] Assigned models: {models_to_run_this_process}")

    processed_model_seq_len_pairs = set()
    if global_summary_csv_path.exists():
        try:
            with global_summary_csv_path.open("r", newline="") as f:
                reader = csv.DictReader(f)
                for row in reader:
                    if row and "model_id" in row and "seq_len" in row:
                        try:
                            processed_model_seq_len_pairs.add((row["model_id"], int(row["seq_len"])))
                        except ValueError:
                            if accelerator.is_main_process:
                                print(f"Warning: Could not parse seq_len from row in summary: {row}")
        except Exception as e:
            if accelerator.is_main_process:
                print(f"Warning: Could not read existing global results from {global_summary_csv_path}: {e}")

    needs_global_header = not global_summary_csv_path.exists() or global_summary_csv_path.stat().st_size == 0
    if accelerator.is_main_process and needs_global_header:
        try:
            with global_summary_csv_path.open("w", newline="") as f:
                writer = csv.writer(f)
                # Updated header to match _04_ structure
                writer.writerow(["model_id", "seq_len", "accuracy_A", "accuracy_B", "accuracy_avg_pair", "num_pairs_evaluated", "num_seqs_requested", "best_of_k"])
            needs_global_header = False
            print(f"[Process {accelerator.process_index}] Global summary CSV header written to {global_summary_csv_path}")
        except IOError as e:
            print(f"[Process {accelerator.process_index}] ERROR: Could not write header to {global_summary_csv_path}: {e}")
            return # Critical error if header cannot be written
    
    accelerator.wait_for_everyone()

    master_rng = np.random.RandomState(12345) # Global RNG for sequence generation

    for model_id in models_to_run_this_process:
        if (model_id, seq_len_opt) in processed_model_seq_len_pairs:
            print(f"[Process {accelerator.process_index}] Skipping model {model_id} for seq_len {seq_len_opt} as it's already in {global_summary_csv_path}.")
            continue

        print(f"[Process {accelerator.process_index}] --- Processing Model: {model_id}, Sequence Length: {seq_len_opt} ---")
        llm = None
        try:
            pipeline_init_kwargs = {
                "trust_remote_code": True,
                "device_map": accelerator.device,
                "torch_dtype": torch.bfloat16,
            }
            # FlashAttention logic can be added here if desired, similar to _04_
            
            tok = AutoTokenizer.from_pretrained(model_id, padding_side="left") # Added padding_side
            llm = pipeline("text-generation", model=model_id, tokenizer=tok, **pipeline_init_kwargs)
            
            print(f"[Process {accelerator.process_index}] Model {model_id} loaded on {llm.device}. Effective model dtype: {llm.model.dtype if llm and hasattr(llm, 'model') else 'N/A'}.")
            # Optional: print attention implementation if relevant
            # if llm and hasattr(llm, 'model') and hasattr(llm.model.config, '_attn_implementation'):
            #      print(f"[Process {accelerator.process_index}] Model {model_id} configured with attention implementation: {llm.model.config._attn_implementation}")

        except Exception as e:
            print(f"[Process {accelerator.process_index}] ERROR: Could not load model {model_id}: {e}. Skipping model.")
            if llm: del llm
            if torch.cuda.is_available(): torch.cuda.empty_cache()
            gc.collect()
            continue
            
        correct_a, correct_b = 0, 0
        total_evaluated_pairs = 0
        all_model_data_for_json = [] # For detailed JSON output

        for i_seq in range(num_seqs):
            divergence_idx = max(0, min(seq_len_opt - 1, seq_len_opt // 2)) # Standard divergence point

            prog_a, prog_b, intermediates_a, intermediates_b = make_counterfactual_pair(
                seq_len_opt, divergence_idx, master_rng 
            )

            if not intermediates_a or not intermediates_b: # Check both
                print(f"  [Process {accelerator.process_index}] WARNING: make_counterfactual_pair returned empty intermediates for model {model_id}, seq {i_seq+1}. Skipping.")
                continue

            true_val_a = intermediates_a[-1]
            true_val_b = intermediates_b[-1]
            
            prompt_text_a = PROMPT_TEMPLATE.format(code=prog_a)
            prompt_text_b = PROMPT_TEMPLATE.format(code=prog_b)
            
            prompts_batch_for_llm = [prompt_text_a, prompt_text_b] # LLM processes one pair at a time

            current_pair_data_for_json = {
                "pair_id": i_seq,
                "seq_len": seq_len_opt,
                "divergence_index": divergence_idx,
                "program_a": prog_a, "intermediates_a": intermediates_a, "true_val_a": true_val_a,
                "program_b": prog_b, "intermediates_b": intermediates_b, "true_val_b": true_val_b,
                "pred_a": None, "pred_b": None, "correct_a": False, "correct_b": False,
                "outputs_a": None, "outputs_b": None # To store raw LLM outputs if needed
            }

            try:
                pipeline_outputs = llm(
                    prompts_batch_for_llm,
                    num_return_sequences=best_of_k_samples,
                    max_new_tokens=10, # Sufficient for integer output
                    do_sample=True if best_of_k_samples > 1 else False,
                    temperature=0.8 if best_of_k_samples > 1 else 0.0, # Matches _04_, though _02_ had 0.8 always
                    batch_size=len(prompts_batch_for_llm) 
                )
            except Exception as e:
                print(f"  [Process {accelerator.process_index}] ERROR: Runtime error during LLM inference for model {model_id}, seq {i_seq+1}: {e}. Skipping pair.")
                if torch.cuda.is_available(): torch.cuda.empty_cache()
                gc.collect()
                continue 

            if len(pipeline_outputs) != len(prompts_batch_for_llm):
                print(f"  [Process {accelerator.process_index}] WARNING: LLM output length mismatch for model {model_id}, seq {i_seq+1}. Expected {len(prompts_batch_for_llm)}, got {len(pipeline_outputs)}. Skipping pair.")
                continue

            # pipeline_outputs is List[List[Dict[str, str]]]
            # First inner list for prog_a, second for prog_b
            outputs_a_list = pipeline_outputs[0]
            outputs_b_list = pipeline_outputs[1]

            # Process Program A
            pred_a = _best_of_k(outputs_a_list, true_val_a, [prompt_text_a] * len(outputs_a_list))
            current_pair_data_for_json["outputs_a"] = outputs_a_list # Store raw outputs
            current_pair_data_for_json["pred_a"] = pred_a
            if pred_a is not None and pred_a == true_val_a:
                correct_a += 1
                current_pair_data_for_json["correct_a"] = True

            # Process Program B
            pred_b = _best_of_k(outputs_b_list, true_val_b, [prompt_text_b] * len(outputs_b_list))
            current_pair_data_for_json["outputs_b"] = outputs_b_list # Store raw outputs
            current_pair_data_for_json["pred_b"] = pred_b
            if pred_b is not None and pred_b == true_val_b:
                correct_b += 1
                current_pair_data_for_json["correct_b"] = True
            
            all_model_data_for_json.append(current_pair_data_for_json)
            total_evaluated_pairs += 1
            
            if (i_seq + 1) % 20 == 0:
                 print(f"  [P{accelerator.process_index}][M {model_id}, SL {seq_len_opt}] Pair {i_seq+1}/{num_seqs}. Acc A: {correct_a/(i_seq+1):.2f}, Acc B: {correct_b/(i_seq+1):.2f}")


        if total_evaluated_pairs > 0:
            acc_a = correct_a / total_evaluated_pairs
            acc_b = correct_b / total_evaluated_pairs
            avg_acc_pair = (correct_a + correct_b) / (total_evaluated_pairs * 2)

            print(f"[Process {accelerator.process_index}] Model {model_id} (SeqLen {seq_len_opt}): Overall Acc A: {acc_a:.2%}, Acc B: {acc_b:.2%}, Avg Pair Acc: {avg_acc_pair:.2%}")
            
            model_name_safe = model_id.replace('/', '_')
            # Detailed path uses detailed_results_dir_for_seq_len
            detailed_path = detailed_results_dir_for_seq_len / f"{model_name_safe}_integer_data.json" # Suffix changed
            with detailed_path.open("w") as f:
                json.dump(all_model_data_for_json, f, indent=2)
            print(f"[Process {accelerator.process_index}] Detailed results for {model_id} (SeqLen {seq_len_opt}) saved to {detailed_path}")

            try:
                # Open in append mode, ensuring synchronization if multiple processes write (though models are split)
                with global_summary_csv_path.open("a", newline="") as f:
                    writer = csv.writer(f)
                    writer.writerow([model_id, seq_len_opt, f"{acc_a:.6f}", f"{acc_b:.6f}", f"{avg_acc_pair:.6f}", total_evaluated_pairs, num_seqs, best_of_k_samples])
                print(f"[Process {accelerator.process_index}] Global summary for {model_id} (SeqLen {seq_len_opt}) appended to {global_summary_csv_path}")
            except IOError as e:
                 print(f"[Process {accelerator.process_index}] ERROR: Could not append to global summary {global_summary_csv_path} for model {model_id}: {e}")
        else:
            print(f"[Process {accelerator.process_index}] No pairs were successfully evaluated for model {model_id} (SeqLen {seq_len_opt}). Attempting to record this in global summary.")
            # Record 0 if no pairs evaluated but model was attempted (and not skipped due to prior completion)
            if not (model_id, seq_len_opt) in processed_model_seq_len_pairs:
                try:
                    with global_summary_csv_path.open("a", newline="") as f:
                        writer = csv.writer(f)
                        writer.writerow([model_id, seq_len_opt, "0.000000", "0.000000", "0.000000", 0, num_seqs, best_of_k_samples])
                    print(f"[Process {accelerator.process_index}] Recorded 0 evaluation for {model_id} (SeqLen {seq_len_opt}) in {global_summary_csv_path}")
                except IOError as e:
                    print(f"[Process {accelerator.process_index}] ERROR: Could not append 0-eval to global summary {global_summary_csv_path} for model {model_id}: {e}")

        if llm: del llm
        if torch.cuda.is_available(): torch.cuda.empty_cache()
        gc.collect()
        print(f"[Process {accelerator.process_index}] --- Finished Model: {model_id} ---")

    accelerator.wait_for_everyone()
    if accelerator.is_main_process:
        print(f"[Process {accelerator.process_index}] All assigned models processed for sequence length {seq_len_opt}.")
        print(f"Summary results for integer task are in {global_summary_csv_path}")

if __name__ == "__main__":
    main()
